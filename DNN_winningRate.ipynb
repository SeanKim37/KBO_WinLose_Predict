{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_winningRate",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 심층신경망\n",
        "# DNN (Deep Neural Network)\n",
        "----\n",
        "#### 모델 저장 : 'DNN_winningRate_model.h5'\n"
      ],
      "metadata": {
        "id": "l56RtSGssVuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyyaml h5py"
      ],
      "metadata": {
        "id": "TAN9nzCWrQcm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping \n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals"
      ],
      "metadata": {
        "id": "-gydC9GU85-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce98472b-12b5-4bb3-e1e2-bfae2870391d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5mqVJ_fr9K8",
        "outputId": "8e7fb5dc-c78d-4e54-f03d-b8232d8dc76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 코랩이랑 연동하기\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xData 불러오기\n",
        "xData_path = \"/content/drive/MyDrive/2022_AI_Web_Hackathon/Data/Data_Processing_xData.csv\"\n",
        "xData = pd.read_csv(xData_path, encoding=\"CP949\")\n",
        "xData = xData.drop(['Unnamed: 0'], axis=1)\n",
        "xData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8OlF504nspjA",
        "outputId": "1abdd103-0e47-47da-dd29-97e541ea1f14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        A0    A1    A2    A3     A4    A5    A6    A7    A8    A9    H0    H1  \\\n",
              "0     2.37  4.41 -1.62  3.34   5.30  0.61  0.15 -1.28 -1.43  0.33  6.35  1.89   \n",
              "1     0.09  3.52  0.84  4.46  11.73  0.76  2.19  0.85  0.25  1.03  1.64  1.94   \n",
              "2     0.36   NaN  1.51  4.68   5.25  1.58 -0.53 -0.50  0.78 -0.21  3.28  4.73   \n",
              "3     1.58  0.99 -1.84  1.76   1.30  2.29 -0.46 -0.22  1.44  1.80  3.12  6.93   \n",
              "4     7.18  2.87  1.33  2.91  -0.56 -0.63 -0.78 -0.24 -0.29 -1.04  3.72  2.51   \n",
              "...    ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "5444  1.75  1.56  0.70  0.62   2.08  2.86  0.49 -0.58 -0.20 -1.06  4.83  0.63   \n",
              "5445  3.56  1.95  2.26  2.61   2.06  1.59  1.38 -0.22  0.29  0.23  1.59  2.24   \n",
              "5446  1.28  2.62  3.45  3.09   3.52  1.22  0.10 -0.41  0.05 -0.27 -1.35  1.55   \n",
              "5447  3.81  3.04  0.57  2.63   1.75 -0.45  1.05 -0.30 -0.65 -0.69  2.00  1.13   \n",
              "5448  0.05  2.24 -0.56  2.02   0.63  0.19 -0.08  0.11  0.30 -0.22  1.20  1.01   \n",
              "\n",
              "        H2    H3    H4    H5    H6    H7    H8    H9  \n",
              "0     2.63  6.90  8.96  2.20  2.37 -0.47  4.09 -0.01  \n",
              "1     1.34  6.13 -0.41  0.01  1.85  5.51  0.00  2.93  \n",
              "2     3.35  3.34  5.05 -2.16  7.03 -1.02  2.88 -0.87  \n",
              "3     2.11  7.05  5.28  3.30  3.97  1.89  0.55  2.08  \n",
              "4    -2.04  2.50  0.58  0.31  3.68  1.27 -1.03 -0.39  \n",
              "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "5444  2.26  4.61  0.78  1.02 -0.30 -0.08  0.57  1.39  \n",
              "5445  0.35  1.39  0.96 -0.37  0.57  0.09 -0.17 -0.04  \n",
              "5446  0.70  4.54  1.94  0.05 -0.07 -0.38 -0.14 -0.47  \n",
              "5447 -0.20  0.31  2.97  2.45  1.44  1.31  0.84  0.59  \n",
              "5448  0.35  0.06  3.94  0.47  1.31 -0.34  0.23 -0.05  \n",
              "\n",
              "[5449 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a2634d-f4a9-4285-9270-da8b9cf3fa5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A0</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>H0</th>\n",
              "      <th>H1</th>\n",
              "      <th>H2</th>\n",
              "      <th>H3</th>\n",
              "      <th>H4</th>\n",
              "      <th>H5</th>\n",
              "      <th>H6</th>\n",
              "      <th>H7</th>\n",
              "      <th>H8</th>\n",
              "      <th>H9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.37</td>\n",
              "      <td>4.41</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.30</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.28</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>0.33</td>\n",
              "      <td>6.35</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2.63</td>\n",
              "      <td>6.90</td>\n",
              "      <td>8.96</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.37</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>4.09</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.84</td>\n",
              "      <td>4.46</td>\n",
              "      <td>11.73</td>\n",
              "      <td>0.76</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.34</td>\n",
              "      <td>6.13</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.85</td>\n",
              "      <td>5.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.51</td>\n",
              "      <td>4.68</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1.58</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.78</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>3.28</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.05</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>7.03</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>2.88</td>\n",
              "      <td>-0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.58</td>\n",
              "      <td>0.99</td>\n",
              "      <td>-1.84</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.30</td>\n",
              "      <td>2.29</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.80</td>\n",
              "      <td>3.12</td>\n",
              "      <td>6.93</td>\n",
              "      <td>2.11</td>\n",
              "      <td>7.05</td>\n",
              "      <td>5.28</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.97</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.55</td>\n",
              "      <td>2.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.18</td>\n",
              "      <td>2.87</td>\n",
              "      <td>1.33</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>3.72</td>\n",
              "      <td>2.51</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.31</td>\n",
              "      <td>3.68</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5444</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.62</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.49</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>4.83</td>\n",
              "      <td>0.63</td>\n",
              "      <td>2.26</td>\n",
              "      <td>4.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5445</th>\n",
              "      <td>3.56</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.26</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.59</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.23</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5446</th>\n",
              "      <td>1.28</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.70</td>\n",
              "      <td>4.54</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5447</th>\n",
              "      <td>3.81</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.57</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.75</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.13</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.31</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>0.05</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.19</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.30</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5449 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a2634d-f4a9-4285-9270-da8b9cf3fa5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24a2634d-f4a9-4285-9270-da8b9cf3fa5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24a2634d-f4a9-4285-9270-da8b9cf3fa5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xData nan 값을 해당하는 열의 평균값으로 변경하기\n",
        "xData_column = ['A0','A1','A2','A3','A4','A5','A6','A7','A8','A9','H0','H1','H2','H3','H4','H5','H6','H7','H8','H9']\n",
        "nanCheck = 0\n",
        "for i in range(len(xData)):\n",
        "    for column in xData_column:\n",
        "        if math.isnan(xData[column][i]):\n",
        "            xData[column][i] = round(xData[column].mean(), 2)\n",
        "            nanCheck += 1\n",
        "print('%d 개의 nan 값을 각 열의 평균값으로 변경 완료했습니다.' %nanCheck)\n",
        "xData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "wFG5HR7JUu9v",
        "outputId": "8eb71523-d95a-4d6e-e58d-df504590a2d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 개의 nan 값을 각 열의 평균값으로 변경 완료했습니다.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        A0    A1    A2    A3     A4    A5    A6    A7    A8    A9    H0    H1  \\\n",
              "0     2.37  4.41 -1.62  3.34   5.30  0.61  0.15 -1.28 -1.43  0.33  6.35  1.89   \n",
              "1     0.09  3.52  0.84  4.46  11.73  0.76  2.19  0.85  0.25  1.03  1.64  1.94   \n",
              "2     0.36  2.00  1.51  4.68   5.25  1.58 -0.53 -0.50  0.78 -0.21  3.28  4.73   \n",
              "3     1.58  0.99 -1.84  1.76   1.30  2.29 -0.46 -0.22  1.44  1.80  3.12  6.93   \n",
              "4     7.18  2.87  1.33  2.91  -0.56 -0.63 -0.78 -0.24 -0.29 -1.04  3.72  2.51   \n",
              "...    ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "5444  1.75  1.56  0.70  0.62   2.08  2.86  0.49 -0.58 -0.20 -1.06  4.83  0.63   \n",
              "5445  3.56  1.95  2.26  2.61   2.06  1.59  1.38 -0.22  0.29  0.23  1.59  2.24   \n",
              "5446  1.28  2.62  3.45  3.09   3.52  1.22  0.10 -0.41  0.05 -0.27 -1.35  1.55   \n",
              "5447  3.81  3.04  0.57  2.63   1.75 -0.45  1.05 -0.30 -0.65 -0.69  2.00  1.13   \n",
              "5448  0.05  2.24 -0.56  2.02   0.63  0.19 -0.08  0.11  0.30 -0.22  1.20  1.01   \n",
              "\n",
              "        H2    H3    H4    H5    H6    H7    H8    H9  \n",
              "0     2.63  6.90  8.96  2.20  2.37 -0.47  4.09 -0.01  \n",
              "1     1.34  6.13 -0.41  0.01  1.85  5.51  0.00  2.93  \n",
              "2     3.35  3.34  5.05 -2.16  7.03 -1.02  2.88 -0.87  \n",
              "3     2.11  7.05  5.28  3.30  3.97  1.89  0.55  2.08  \n",
              "4    -2.04  2.50  0.58  0.31  3.68  1.27 -1.03 -0.39  \n",
              "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "5444  2.26  4.61  0.78  1.02 -0.30 -0.08  0.57  1.39  \n",
              "5445  0.35  1.39  0.96 -0.37  0.57  0.09 -0.17 -0.04  \n",
              "5446  0.70  4.54  1.94  0.05 -0.07 -0.38 -0.14 -0.47  \n",
              "5447 -0.20  0.31  2.97  2.45  1.44  1.31  0.84  0.59  \n",
              "5448  0.35  0.06  3.94  0.47  1.31 -0.34  0.23 -0.05  \n",
              "\n",
              "[5449 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c8949b1-3a0a-4dea-bd47-559d8cf3372e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A0</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>H0</th>\n",
              "      <th>H1</th>\n",
              "      <th>H2</th>\n",
              "      <th>H3</th>\n",
              "      <th>H4</th>\n",
              "      <th>H5</th>\n",
              "      <th>H6</th>\n",
              "      <th>H7</th>\n",
              "      <th>H8</th>\n",
              "      <th>H9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.37</td>\n",
              "      <td>4.41</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.30</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.28</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>0.33</td>\n",
              "      <td>6.35</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2.63</td>\n",
              "      <td>6.90</td>\n",
              "      <td>8.96</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.37</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>4.09</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.84</td>\n",
              "      <td>4.46</td>\n",
              "      <td>11.73</td>\n",
              "      <td>0.76</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.34</td>\n",
              "      <td>6.13</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.85</td>\n",
              "      <td>5.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.36</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.51</td>\n",
              "      <td>4.68</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1.58</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.78</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>3.28</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.05</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>7.03</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>2.88</td>\n",
              "      <td>-0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.58</td>\n",
              "      <td>0.99</td>\n",
              "      <td>-1.84</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.30</td>\n",
              "      <td>2.29</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.80</td>\n",
              "      <td>3.12</td>\n",
              "      <td>6.93</td>\n",
              "      <td>2.11</td>\n",
              "      <td>7.05</td>\n",
              "      <td>5.28</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.97</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.55</td>\n",
              "      <td>2.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.18</td>\n",
              "      <td>2.87</td>\n",
              "      <td>1.33</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>3.72</td>\n",
              "      <td>2.51</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.31</td>\n",
              "      <td>3.68</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5444</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.62</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.49</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>4.83</td>\n",
              "      <td>0.63</td>\n",
              "      <td>2.26</td>\n",
              "      <td>4.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5445</th>\n",
              "      <td>3.56</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.26</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.59</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.23</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5446</th>\n",
              "      <td>1.28</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.70</td>\n",
              "      <td>4.54</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5447</th>\n",
              "      <td>3.81</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.57</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.75</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.13</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.31</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>0.05</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.19</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.30</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5449 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c8949b1-3a0a-4dea-bd47-559d8cf3372e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c8949b1-3a0a-4dea-bd47-559d8cf3372e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c8949b1-3a0a-4dea-bd47-559d8cf3372e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xData에서 Away랑 Home의 순서를 바꿔 데이터 추가(X2)\n",
        "xData_num = len(xData)\n",
        "for i in range(xData_num):\n",
        "    xData_insert = {\n",
        "        \"A0\": xData[\"H0\"][i], \"A1\": xData[\"H1\"][i], \"A2\": xData[\"H2\"][i], \"A3\": xData[\"H3\"][i],\n",
        "        \"A4\": xData[\"H4\"][i], \"A5\": xData[\"H5\"][i], \"A6\": xData[\"H6\"][i], \"A7\": xData[\"H7\"][i],\n",
        "        \"A8\": xData[\"H8\"][i], \"A9\": xData[\"H9\"][i], \"H0\": xData[\"A0\"][i], \"H1\": xData[\"A1\"][i],\n",
        "        \"H2\": xData[\"A2\"][i], \"H3\": xData[\"A3\"][i], \"H4\": xData[\"A4\"][i], \"H5\": xData[\"A5\"][i],\n",
        "        \"H6\": xData[\"A6\"][i], \"H7\": xData[\"A7\"][i], \"H8\": xData[\"A8\"][i], \"H9\": xData[\"A9\"][i]\n",
        "    }\n",
        "    xData = xData.append(xData_insert, ignore_index=True)\n",
        "xData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iIdkHO9gfa2P",
        "outputId": "c9a753df-0ff0-48b8-ba7e-a5376779fdfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         A0    A1    A2    A3     A4    A5    A6    A7    A8    A9    H0  \\\n",
              "0      2.37  4.41 -1.62  3.34   5.30  0.61  0.15 -1.28 -1.43  0.33  6.35   \n",
              "1      0.09  3.52  0.84  4.46  11.73  0.76  2.19  0.85  0.25  1.03  1.64   \n",
              "2      0.36  2.00  1.51  4.68   5.25  1.58 -0.53 -0.50  0.78 -0.21  3.28   \n",
              "3      1.58  0.99 -1.84  1.76   1.30  2.29 -0.46 -0.22  1.44  1.80  3.12   \n",
              "4      7.18  2.87  1.33  2.91  -0.56 -0.63 -0.78 -0.24 -0.29 -1.04  3.72   \n",
              "...     ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   \n",
              "10893  4.83  0.63  2.26  4.61   0.78  1.02 -0.30 -0.08  0.57  1.39  1.75   \n",
              "10894  1.59  2.24  0.35  1.39   0.96 -0.37  0.57  0.09 -0.17 -0.04  3.56   \n",
              "10895 -1.35  1.55  0.70  4.54   1.94  0.05 -0.07 -0.38 -0.14 -0.47  1.28   \n",
              "10896  2.00  1.13 -0.20  0.31   2.97  2.45  1.44  1.31  0.84  0.59  3.81   \n",
              "10897  1.20  1.01  0.35  0.06   3.94  0.47  1.31 -0.34  0.23 -0.05  0.05   \n",
              "\n",
              "         H1    H2    H3    H4    H5    H6    H7    H8    H9  \n",
              "0      1.89  2.63  6.90  8.96  2.20  2.37 -0.47  4.09 -0.01  \n",
              "1      1.94  1.34  6.13 -0.41  0.01  1.85  5.51  0.00  2.93  \n",
              "2      4.73  3.35  3.34  5.05 -2.16  7.03 -1.02  2.88 -0.87  \n",
              "3      6.93  2.11  7.05  5.28  3.30  3.97  1.89  0.55  2.08  \n",
              "4      2.51 -2.04  2.50  0.58  0.31  3.68  1.27 -1.03 -0.39  \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "10893  1.56  0.70  0.62  2.08  2.86  0.49 -0.58 -0.20 -1.06  \n",
              "10894  1.95  2.26  2.61  2.06  1.59  1.38 -0.22  0.29  0.23  \n",
              "10895  2.62  3.45  3.09  3.52  1.22  0.10 -0.41  0.05 -0.27  \n",
              "10896  3.04  0.57  2.63  1.75 -0.45  1.05 -0.30 -0.65 -0.69  \n",
              "10897  2.24 -0.56  2.02  0.63  0.19 -0.08  0.11  0.30 -0.22  \n",
              "\n",
              "[10898 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f77130d-2935-43de-92d0-5304a004fbf6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A0</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>H0</th>\n",
              "      <th>H1</th>\n",
              "      <th>H2</th>\n",
              "      <th>H3</th>\n",
              "      <th>H4</th>\n",
              "      <th>H5</th>\n",
              "      <th>H6</th>\n",
              "      <th>H7</th>\n",
              "      <th>H8</th>\n",
              "      <th>H9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.37</td>\n",
              "      <td>4.41</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.30</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.28</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>0.33</td>\n",
              "      <td>6.35</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2.63</td>\n",
              "      <td>6.90</td>\n",
              "      <td>8.96</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.37</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>4.09</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.84</td>\n",
              "      <td>4.46</td>\n",
              "      <td>11.73</td>\n",
              "      <td>0.76</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.34</td>\n",
              "      <td>6.13</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.85</td>\n",
              "      <td>5.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.36</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.51</td>\n",
              "      <td>4.68</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1.58</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.78</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>3.28</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.34</td>\n",
              "      <td>5.05</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>7.03</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>2.88</td>\n",
              "      <td>-0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.58</td>\n",
              "      <td>0.99</td>\n",
              "      <td>-1.84</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.30</td>\n",
              "      <td>2.29</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.80</td>\n",
              "      <td>3.12</td>\n",
              "      <td>6.93</td>\n",
              "      <td>2.11</td>\n",
              "      <td>7.05</td>\n",
              "      <td>5.28</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.97</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.55</td>\n",
              "      <td>2.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.18</td>\n",
              "      <td>2.87</td>\n",
              "      <td>1.33</td>\n",
              "      <td>2.91</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>3.72</td>\n",
              "      <td>2.51</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.31</td>\n",
              "      <td>3.68</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10893</th>\n",
              "      <td>4.83</td>\n",
              "      <td>0.63</td>\n",
              "      <td>2.26</td>\n",
              "      <td>4.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.39</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.62</td>\n",
              "      <td>2.08</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.49</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-1.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10894</th>\n",
              "      <td>1.59</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>3.56</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.26</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.06</td>\n",
              "      <td>1.59</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10895</th>\n",
              "      <td>-1.35</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.70</td>\n",
              "      <td>4.54</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>1.28</td>\n",
              "      <td>2.62</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10896</th>\n",
              "      <td>2.00</td>\n",
              "      <td>1.13</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.31</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.59</td>\n",
              "      <td>3.81</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.57</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.75</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10897</th>\n",
              "      <td>1.20</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.19</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.30</td>\n",
              "      <td>-0.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10898 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f77130d-2935-43de-92d0-5304a004fbf6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f77130d-2935-43de-92d0-5304a004fbf6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f77130d-2935-43de-92d0-5304a004fbf6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yData 불러오기\n",
        "yData_path = \"/content/drive/MyDrive/2022_AI_Web_Hackathon/Data/Data_Processing_yData.csv\"\n",
        "yData = pd.read_csv(yData_path, encoding=\"CP949\")\n",
        "yData = yData.drop(['Unnamed: 0'], axis=1)\n",
        "yData"
      ],
      "metadata": {
        "id": "DJq6XHUltbQ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "915b42c9-0f70-4ef5-c3fb-df3c3285f4ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Result\n",
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "...      ...\n",
              "5444       0\n",
              "5445       1\n",
              "5446       1\n",
              "5447       1\n",
              "5448       0\n",
              "\n",
              "[5449 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8192aa0-6f51-43ca-9a8a-0a1de25409b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5444</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5445</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5446</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5447</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5449 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8192aa0-6f51-43ca-9a8a-0a1de25409b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8192aa0-6f51-43ca-9a8a-0a1de25409b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8192aa0-6f51-43ca-9a8a-0a1de25409b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yData에서 Away랑 Home의 순서를 바꿔 데이터 추가(X2)\n",
        "yData_num = len(yData)\n",
        "for i in range(yData_num):\n",
        "    if str(yData[\"Result\"][i]) == '0':\n",
        "        new_result = 1\n",
        "    else:\n",
        "        new_result = 0\n",
        "    yData_insert = {\n",
        "        \"Result\": new_result\n",
        "    }\n",
        "    yData = yData.append(yData_insert, ignore_index=True)\n",
        "yData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LKnHwgGIjazj",
        "outputId": "1f6d4d73-8f77-4dd2-9fb4-de8825822f81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Result\n",
              "0           0\n",
              "1           0\n",
              "2           0\n",
              "3           0\n",
              "4           0\n",
              "...       ...\n",
              "10893       1\n",
              "10894       0\n",
              "10895       0\n",
              "10896       0\n",
              "10897       1\n",
              "\n",
              "[10898 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1dfd9ff-9830-477b-bcb3-927cb61ad58b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10893</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10894</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10895</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10896</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10897</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10898 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1dfd9ff-9830-477b-bcb3-927cb61ad58b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1dfd9ff-9830-477b-bcb3-927cb61ad58b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1dfd9ff-9830-477b-bcb3-927cb61ad58b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xData yData 의 values 확인하기\n",
        "print('xData.values')\n",
        "print(xData.values)\n",
        "\n",
        "print('\\nyData.values')\n",
        "print(yData.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R29wMlHmcRdz",
        "outputId": "0810257d-099d-4fda-b4ec-549d37f973d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xData.values\n",
            "[[ 2.37  4.41 -1.62 ... -0.47  4.09 -0.01]\n",
            " [ 0.09  3.52  0.84 ...  5.51  0.    2.93]\n",
            " [ 0.36  2.    1.51 ... -1.02  2.88 -0.87]\n",
            " ...\n",
            " [-1.35  1.55  0.7  ... -0.41  0.05 -0.27]\n",
            " [ 2.    1.13 -0.2  ... -0.3  -0.65 -0.69]\n",
            " [ 1.2   1.01  0.35 ...  0.11  0.3  -0.22]]\n",
            "\n",
            "yData.values\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (train / validation / test) dataset 설정\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(xData, yData, test_size=0.05, random_state=1)\n",
        "val_split_num = len(x_train_val) - (len(x_test))\n",
        "\n",
        "x_train = x_train_val[:val_split_num]\n",
        "x_valid = x_train_val[val_split_num:]\n",
        "y_train = y_train_val[:val_split_num]\n",
        "y_valid = y_train_val[val_split_num:]\n",
        "\n",
        "print(\"x_train =\", x_train.shape, type(x_train))\n",
        "print(\"y_train =\", y_train.shape, type(y_test))\n",
        "print(\"\\nx_valid =\", x_valid.shape, type(x_valid))\n",
        "print(\"y_valid =\", y_valid.shape, type(y_valid))\n",
        "print(\"\\nx_test  =\", x_test.shape, type(x_test))\n",
        "print(\"y_test  =\", y_test.shape, type(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V556K_U5rGfq",
        "outputId": "05bdb8ae-23fe-4c64-f9d9-f482603e2bc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train = (9808, 20) <class 'pandas.core.frame.DataFrame'>\n",
            "y_train = (9808, 1) <class 'pandas.core.frame.DataFrame'>\n",
            "\n",
            "x_valid = (545, 20) <class 'pandas.core.frame.DataFrame'>\n",
            "y_valid = (545, 1) <class 'pandas.core.frame.DataFrame'>\n",
            "\n",
            "x_test  = (545, 20) <class 'pandas.core.frame.DataFrame'>\n",
            "y_test  = (545, 1) <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN model 설정하기\n",
        "input_shape = [x_train.shape[1]]\n",
        "dense1 = keras.layers.Dense(100, activation='relu', input_shape=input_shape)\n",
        "dense2 = keras.layers.Dense(100, activation='relu')\n",
        "dense3 = keras.layers.Dense(10, activation='relu')\n",
        "dense4 = keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "model = keras.Sequential([dense1, dense2, dense3, dense4])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GxyQtVOhdJJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b209f7-07d9-4714-87dd-d570ab96b0c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               2100      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,221\n",
            "Trainable params: 13,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN 실행하기\n",
        "batch = 100\n",
        "epoch = 2000\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 30, mode = 'auto')\n",
        "\n",
        "hist = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = epoch,\n",
        "    batch_size = batch,\n",
        "    validation_data = (x_valid, y_valid),\n",
        "    validation_batch_size = batch,\n",
        "    callbacks = [early_stopping]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJocVRuMdK2o",
        "outputId": "339de655-2931-4b20-ad38-50e8c666c8eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "99/99 [==============================] - 4s 5ms/step - loss: 0.8009 - accuracy: 0.4883 - val_loss: 0.8093 - val_accuracy: 0.4954\n",
            "Epoch 2/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7934 - accuracy: 0.4887 - val_loss: 0.8015 - val_accuracy: 0.4917\n",
            "Epoch 3/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7865 - accuracy: 0.4881 - val_loss: 0.7944 - val_accuracy: 0.4899\n",
            "Epoch 4/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7801 - accuracy: 0.4880 - val_loss: 0.7876 - val_accuracy: 0.4862\n",
            "Epoch 5/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7742 - accuracy: 0.4871 - val_loss: 0.7813 - val_accuracy: 0.4917\n",
            "Epoch 6/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7689 - accuracy: 0.4859 - val_loss: 0.7757 - val_accuracy: 0.4917\n",
            "Epoch 7/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7639 - accuracy: 0.4873 - val_loss: 0.7704 - val_accuracy: 0.4936\n",
            "Epoch 8/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7593 - accuracy: 0.4865 - val_loss: 0.7652 - val_accuracy: 0.4881\n",
            "Epoch 9/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7550 - accuracy: 0.4872 - val_loss: 0.7604 - val_accuracy: 0.4862\n",
            "Epoch 10/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7510 - accuracy: 0.4881 - val_loss: 0.7558 - val_accuracy: 0.4899\n",
            "Epoch 11/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7472 - accuracy: 0.4866 - val_loss: 0.7515 - val_accuracy: 0.4881\n",
            "Epoch 12/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7438 - accuracy: 0.4867 - val_loss: 0.7475 - val_accuracy: 0.4899\n",
            "Epoch 13/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7406 - accuracy: 0.4863 - val_loss: 0.7438 - val_accuracy: 0.4936\n",
            "Epoch 14/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7376 - accuracy: 0.4853 - val_loss: 0.7403 - val_accuracy: 0.4991\n",
            "Epoch 15/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7349 - accuracy: 0.4849 - val_loss: 0.7371 - val_accuracy: 0.5009\n",
            "Epoch 16/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7323 - accuracy: 0.4832 - val_loss: 0.7340 - val_accuracy: 0.5009\n",
            "Epoch 17/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7298 - accuracy: 0.4842 - val_loss: 0.7311 - val_accuracy: 0.5083\n",
            "Epoch 18/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7276 - accuracy: 0.4852 - val_loss: 0.7285 - val_accuracy: 0.5101\n",
            "Epoch 19/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7255 - accuracy: 0.4837 - val_loss: 0.7260 - val_accuracy: 0.5101\n",
            "Epoch 20/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7235 - accuracy: 0.4846 - val_loss: 0.7235 - val_accuracy: 0.5211\n",
            "Epoch 21/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7217 - accuracy: 0.4849 - val_loss: 0.7213 - val_accuracy: 0.5174\n",
            "Epoch 22/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7200 - accuracy: 0.4854 - val_loss: 0.7192 - val_accuracy: 0.5211\n",
            "Epoch 23/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7184 - accuracy: 0.4857 - val_loss: 0.7174 - val_accuracy: 0.5303\n",
            "Epoch 24/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7169 - accuracy: 0.4877 - val_loss: 0.7156 - val_accuracy: 0.5229\n",
            "Epoch 25/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7154 - accuracy: 0.4889 - val_loss: 0.7140 - val_accuracy: 0.5248\n",
            "Epoch 26/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7141 - accuracy: 0.4909 - val_loss: 0.7124 - val_accuracy: 0.5284\n",
            "Epoch 27/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7129 - accuracy: 0.4909 - val_loss: 0.7110 - val_accuracy: 0.5303\n",
            "Epoch 28/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.7117 - accuracy: 0.4919 - val_loss: 0.7096 - val_accuracy: 0.5303\n",
            "Epoch 29/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.7106 - accuracy: 0.4932 - val_loss: 0.7082 - val_accuracy: 0.5266\n",
            "Epoch 30/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7096 - accuracy: 0.4913 - val_loss: 0.7070 - val_accuracy: 0.5266\n",
            "Epoch 31/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7087 - accuracy: 0.4919 - val_loss: 0.7058 - val_accuracy: 0.5284\n",
            "Epoch 32/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7078 - accuracy: 0.4921 - val_loss: 0.7047 - val_accuracy: 0.5284\n",
            "Epoch 33/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7069 - accuracy: 0.4929 - val_loss: 0.7037 - val_accuracy: 0.5266\n",
            "Epoch 34/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7061 - accuracy: 0.4936 - val_loss: 0.7027 - val_accuracy: 0.5321\n",
            "Epoch 35/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7054 - accuracy: 0.4946 - val_loss: 0.7018 - val_accuracy: 0.5339\n",
            "Epoch 36/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7046 - accuracy: 0.4955 - val_loss: 0.7009 - val_accuracy: 0.5376\n",
            "Epoch 37/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7039 - accuracy: 0.4976 - val_loss: 0.7001 - val_accuracy: 0.5358\n",
            "Epoch 38/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7032 - accuracy: 0.4992 - val_loss: 0.6993 - val_accuracy: 0.5394\n",
            "Epoch 39/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7025 - accuracy: 0.4997 - val_loss: 0.6985 - val_accuracy: 0.5413\n",
            "Epoch 40/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7019 - accuracy: 0.5010 - val_loss: 0.6978 - val_accuracy: 0.5413\n",
            "Epoch 41/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.7013 - accuracy: 0.5015 - val_loss: 0.6971 - val_accuracy: 0.5413\n",
            "Epoch 42/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.7008 - accuracy: 0.5014 - val_loss: 0.6965 - val_accuracy: 0.5413\n",
            "Epoch 43/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.7002 - accuracy: 0.5016 - val_loss: 0.6958 - val_accuracy: 0.5413\n",
            "Epoch 44/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6997 - accuracy: 0.5031 - val_loss: 0.6951 - val_accuracy: 0.5431\n",
            "Epoch 45/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6991 - accuracy: 0.5033 - val_loss: 0.6945 - val_accuracy: 0.5450\n",
            "Epoch 46/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6986 - accuracy: 0.5042 - val_loss: 0.6939 - val_accuracy: 0.5541\n",
            "Epoch 47/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6981 - accuracy: 0.5048 - val_loss: 0.6933 - val_accuracy: 0.5505\n",
            "Epoch 48/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6976 - accuracy: 0.5061 - val_loss: 0.6927 - val_accuracy: 0.5505\n",
            "Epoch 49/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6971 - accuracy: 0.5077 - val_loss: 0.6922 - val_accuracy: 0.5486\n",
            "Epoch 50/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6967 - accuracy: 0.5095 - val_loss: 0.6917 - val_accuracy: 0.5523\n",
            "Epoch 51/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6962 - accuracy: 0.5108 - val_loss: 0.6912 - val_accuracy: 0.5523\n",
            "Epoch 52/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6958 - accuracy: 0.5132 - val_loss: 0.6907 - val_accuracy: 0.5596\n",
            "Epoch 53/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6954 - accuracy: 0.5143 - val_loss: 0.6903 - val_accuracy: 0.5633\n",
            "Epoch 54/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6950 - accuracy: 0.5142 - val_loss: 0.6899 - val_accuracy: 0.5615\n",
            "Epoch 55/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6947 - accuracy: 0.5166 - val_loss: 0.6895 - val_accuracy: 0.5615\n",
            "Epoch 56/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6943 - accuracy: 0.5170 - val_loss: 0.6891 - val_accuracy: 0.5615\n",
            "Epoch 57/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6939 - accuracy: 0.5181 - val_loss: 0.6887 - val_accuracy: 0.5560\n",
            "Epoch 58/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5191 - val_loss: 0.6884 - val_accuracy: 0.5541\n",
            "Epoch 59/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.5210 - val_loss: 0.6880 - val_accuracy: 0.5505\n",
            "Epoch 60/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6930 - accuracy: 0.5222 - val_loss: 0.6876 - val_accuracy: 0.5505\n",
            "Epoch 61/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6927 - accuracy: 0.5230 - val_loss: 0.6872 - val_accuracy: 0.5486\n",
            "Epoch 62/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5254 - val_loss: 0.6868 - val_accuracy: 0.5486\n",
            "Epoch 63/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6921 - accuracy: 0.5271 - val_loss: 0.6865 - val_accuracy: 0.5541\n",
            "Epoch 64/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6918 - accuracy: 0.5265 - val_loss: 0.6861 - val_accuracy: 0.5523\n",
            "Epoch 65/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5280 - val_loss: 0.6858 - val_accuracy: 0.5541\n",
            "Epoch 66/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5283 - val_loss: 0.6854 - val_accuracy: 0.5578\n",
            "Epoch 67/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5299 - val_loss: 0.6851 - val_accuracy: 0.5596\n",
            "Epoch 68/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5320 - val_loss: 0.6848 - val_accuracy: 0.5578\n",
            "Epoch 69/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5325 - val_loss: 0.6844 - val_accuracy: 0.5615\n",
            "Epoch 70/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5327 - val_loss: 0.6841 - val_accuracy: 0.5670\n",
            "Epoch 71/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5332 - val_loss: 0.6838 - val_accuracy: 0.5688\n",
            "Epoch 72/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5333 - val_loss: 0.6834 - val_accuracy: 0.5670\n",
            "Epoch 73/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5347 - val_loss: 0.6831 - val_accuracy: 0.5688\n",
            "Epoch 74/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5355 - val_loss: 0.6828 - val_accuracy: 0.5725\n",
            "Epoch 75/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5358 - val_loss: 0.6824 - val_accuracy: 0.5761\n",
            "Epoch 76/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.5368 - val_loss: 0.6821 - val_accuracy: 0.5761\n",
            "Epoch 77/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5372 - val_loss: 0.6818 - val_accuracy: 0.5743\n",
            "Epoch 78/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5380 - val_loss: 0.6815 - val_accuracy: 0.5743\n",
            "Epoch 79/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5384 - val_loss: 0.6812 - val_accuracy: 0.5743\n",
            "Epoch 80/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5389 - val_loss: 0.6809 - val_accuracy: 0.5743\n",
            "Epoch 81/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5398 - val_loss: 0.6806 - val_accuracy: 0.5761\n",
            "Epoch 82/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5401 - val_loss: 0.6803 - val_accuracy: 0.5798\n",
            "Epoch 83/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5408 - val_loss: 0.6800 - val_accuracy: 0.5817\n",
            "Epoch 84/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5410 - val_loss: 0.6797 - val_accuracy: 0.5798\n",
            "Epoch 85/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5417 - val_loss: 0.6794 - val_accuracy: 0.5798\n",
            "Epoch 86/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5425 - val_loss: 0.6792 - val_accuracy: 0.5835\n",
            "Epoch 87/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5429 - val_loss: 0.6789 - val_accuracy: 0.5908\n",
            "Epoch 88/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5430 - val_loss: 0.6786 - val_accuracy: 0.5927\n",
            "Epoch 89/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.5446 - val_loss: 0.6784 - val_accuracy: 0.5945\n",
            "Epoch 90/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5462 - val_loss: 0.6781 - val_accuracy: 0.5927\n",
            "Epoch 91/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5464 - val_loss: 0.6778 - val_accuracy: 0.5908\n",
            "Epoch 92/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6857 - accuracy: 0.5474 - val_loss: 0.6776 - val_accuracy: 0.5908\n",
            "Epoch 93/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.5481 - val_loss: 0.6773 - val_accuracy: 0.5890\n",
            "Epoch 94/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5482 - val_loss: 0.6771 - val_accuracy: 0.5908\n",
            "Epoch 95/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5486 - val_loss: 0.6769 - val_accuracy: 0.5945\n",
            "Epoch 96/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5494 - val_loss: 0.6766 - val_accuracy: 0.5945\n",
            "Epoch 97/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5500 - val_loss: 0.6764 - val_accuracy: 0.5945\n",
            "Epoch 98/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5515 - val_loss: 0.6761 - val_accuracy: 0.5963\n",
            "Epoch 99/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.5522 - val_loss: 0.6759 - val_accuracy: 0.5982\n",
            "Epoch 100/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5530 - val_loss: 0.6756 - val_accuracy: 0.5982\n",
            "Epoch 101/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.5531 - val_loss: 0.6754 - val_accuracy: 0.6018\n",
            "Epoch 102/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5537 - val_loss: 0.6752 - val_accuracy: 0.6000\n",
            "Epoch 103/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6839 - accuracy: 0.5542 - val_loss: 0.6750 - val_accuracy: 0.6018\n",
            "Epoch 104/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5548 - val_loss: 0.6748 - val_accuracy: 0.6018\n",
            "Epoch 105/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.5544 - val_loss: 0.6746 - val_accuracy: 0.6037\n",
            "Epoch 106/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.5551 - val_loss: 0.6744 - val_accuracy: 0.6055\n",
            "Epoch 107/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.5553 - val_loss: 0.6741 - val_accuracy: 0.6055\n",
            "Epoch 108/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5562 - val_loss: 0.6739 - val_accuracy: 0.6037\n",
            "Epoch 109/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6830 - accuracy: 0.5564 - val_loss: 0.6737 - val_accuracy: 0.6055\n",
            "Epoch 110/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6829 - accuracy: 0.5569 - val_loss: 0.6735 - val_accuracy: 0.6037\n",
            "Epoch 111/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6827 - accuracy: 0.5573 - val_loss: 0.6733 - val_accuracy: 0.6037\n",
            "Epoch 112/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5576 - val_loss: 0.6732 - val_accuracy: 0.6037\n",
            "Epoch 113/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6825 - accuracy: 0.5579 - val_loss: 0.6730 - val_accuracy: 0.6055\n",
            "Epoch 114/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5588 - val_loss: 0.6728 - val_accuracy: 0.6073\n",
            "Epoch 115/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.5585 - val_loss: 0.6726 - val_accuracy: 0.6055\n",
            "Epoch 116/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6821 - accuracy: 0.5593 - val_loss: 0.6724 - val_accuracy: 0.6018\n",
            "Epoch 117/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6819 - accuracy: 0.5601 - val_loss: 0.6723 - val_accuracy: 0.6037\n",
            "Epoch 118/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.5610 - val_loss: 0.6721 - val_accuracy: 0.6018\n",
            "Epoch 119/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.5617 - val_loss: 0.6719 - val_accuracy: 0.6018\n",
            "Epoch 120/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6815 - accuracy: 0.5625 - val_loss: 0.6718 - val_accuracy: 0.6018\n",
            "Epoch 121/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.5628 - val_loss: 0.6716 - val_accuracy: 0.6037\n",
            "Epoch 122/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6813 - accuracy: 0.5633 - val_loss: 0.6714 - val_accuracy: 0.6055\n",
            "Epoch 123/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5639 - val_loss: 0.6713 - val_accuracy: 0.6055\n",
            "Epoch 124/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6811 - accuracy: 0.5646 - val_loss: 0.6711 - val_accuracy: 0.6073\n",
            "Epoch 125/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6809 - accuracy: 0.5648 - val_loss: 0.6710 - val_accuracy: 0.6092\n",
            "Epoch 126/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5649 - val_loss: 0.6708 - val_accuracy: 0.6073\n",
            "Epoch 127/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5658 - val_loss: 0.6707 - val_accuracy: 0.6055\n",
            "Epoch 128/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.5661 - val_loss: 0.6705 - val_accuracy: 0.6055\n",
            "Epoch 129/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.5657 - val_loss: 0.6703 - val_accuracy: 0.6073\n",
            "Epoch 130/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5649 - val_loss: 0.6702 - val_accuracy: 0.6073\n",
            "Epoch 131/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6802 - accuracy: 0.5642 - val_loss: 0.6700 - val_accuracy: 0.6073\n",
            "Epoch 132/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6801 - accuracy: 0.5642 - val_loss: 0.6699 - val_accuracy: 0.6092\n",
            "Epoch 133/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6800 - accuracy: 0.5652 - val_loss: 0.6698 - val_accuracy: 0.6092\n",
            "Epoch 134/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.5657 - val_loss: 0.6696 - val_accuracy: 0.6092\n",
            "Epoch 135/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5666 - val_loss: 0.6695 - val_accuracy: 0.6110\n",
            "Epoch 136/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.5669 - val_loss: 0.6693 - val_accuracy: 0.6092\n",
            "Epoch 137/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.5670 - val_loss: 0.6692 - val_accuracy: 0.6092\n",
            "Epoch 138/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6795 - accuracy: 0.5668 - val_loss: 0.6691 - val_accuracy: 0.6092\n",
            "Epoch 139/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.5668 - val_loss: 0.6689 - val_accuracy: 0.6092\n",
            "Epoch 140/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6793 - accuracy: 0.5677 - val_loss: 0.6688 - val_accuracy: 0.6092\n",
            "Epoch 141/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6792 - accuracy: 0.5681 - val_loss: 0.6687 - val_accuracy: 0.6092\n",
            "Epoch 142/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.5689 - val_loss: 0.6686 - val_accuracy: 0.6110\n",
            "Epoch 143/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6790 - accuracy: 0.5696 - val_loss: 0.6684 - val_accuracy: 0.6128\n",
            "Epoch 144/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.5697 - val_loss: 0.6683 - val_accuracy: 0.6128\n",
            "Epoch 145/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.5699 - val_loss: 0.6682 - val_accuracy: 0.6128\n",
            "Epoch 146/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.5699 - val_loss: 0.6681 - val_accuracy: 0.6128\n",
            "Epoch 147/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6786 - accuracy: 0.5705 - val_loss: 0.6679 - val_accuracy: 0.6128\n",
            "Epoch 148/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.5700 - val_loss: 0.6678 - val_accuracy: 0.6147\n",
            "Epoch 149/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6784 - accuracy: 0.5715 - val_loss: 0.6677 - val_accuracy: 0.6165\n",
            "Epoch 150/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6783 - accuracy: 0.5708 - val_loss: 0.6676 - val_accuracy: 0.6183\n",
            "Epoch 151/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6782 - accuracy: 0.5702 - val_loss: 0.6675 - val_accuracy: 0.6147\n",
            "Epoch 152/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6781 - accuracy: 0.5718 - val_loss: 0.6674 - val_accuracy: 0.6147\n",
            "Epoch 153/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.5726 - val_loss: 0.6672 - val_accuracy: 0.6147\n",
            "Epoch 154/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.5732 - val_loss: 0.6671 - val_accuracy: 0.6147\n",
            "Epoch 155/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6778 - accuracy: 0.5728 - val_loss: 0.6670 - val_accuracy: 0.6147\n",
            "Epoch 156/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.5729 - val_loss: 0.6669 - val_accuracy: 0.6128\n",
            "Epoch 157/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.5732 - val_loss: 0.6668 - val_accuracy: 0.6128\n",
            "Epoch 158/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6775 - accuracy: 0.5735 - val_loss: 0.6667 - val_accuracy: 0.6128\n",
            "Epoch 159/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6774 - accuracy: 0.5741 - val_loss: 0.6666 - val_accuracy: 0.6128\n",
            "Epoch 160/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5737 - val_loss: 0.6665 - val_accuracy: 0.6147\n",
            "Epoch 161/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.5732 - val_loss: 0.6664 - val_accuracy: 0.6128\n",
            "Epoch 162/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.5734 - val_loss: 0.6663 - val_accuracy: 0.6128\n",
            "Epoch 163/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6771 - accuracy: 0.5739 - val_loss: 0.6662 - val_accuracy: 0.6128\n",
            "Epoch 164/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.5743 - val_loss: 0.6660 - val_accuracy: 0.6092\n",
            "Epoch 165/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6769 - accuracy: 0.5747 - val_loss: 0.6659 - val_accuracy: 0.6073\n",
            "Epoch 166/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6768 - accuracy: 0.5744 - val_loss: 0.6658 - val_accuracy: 0.6055\n",
            "Epoch 167/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6767 - accuracy: 0.5748 - val_loss: 0.6657 - val_accuracy: 0.6055\n",
            "Epoch 168/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.5744 - val_loss: 0.6656 - val_accuracy: 0.6037\n",
            "Epoch 169/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.5746 - val_loss: 0.6655 - val_accuracy: 0.6018\n",
            "Epoch 170/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.5751 - val_loss: 0.6654 - val_accuracy: 0.6037\n",
            "Epoch 171/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6764 - accuracy: 0.5754 - val_loss: 0.6653 - val_accuracy: 0.6037\n",
            "Epoch 172/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.5756 - val_loss: 0.6652 - val_accuracy: 0.6037\n",
            "Epoch 173/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.5750 - val_loss: 0.6651 - val_accuracy: 0.6018\n",
            "Epoch 174/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.5743 - val_loss: 0.6650 - val_accuracy: 0.6018\n",
            "Epoch 175/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.5746 - val_loss: 0.6649 - val_accuracy: 0.6018\n",
            "Epoch 176/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6760 - accuracy: 0.5744 - val_loss: 0.6648 - val_accuracy: 0.6018\n",
            "Epoch 177/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6759 - accuracy: 0.5743 - val_loss: 0.6647 - val_accuracy: 0.6000\n",
            "Epoch 178/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.5750 - val_loss: 0.6646 - val_accuracy: 0.6018\n",
            "Epoch 179/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.5751 - val_loss: 0.6645 - val_accuracy: 0.6018\n",
            "Epoch 180/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6757 - accuracy: 0.5749 - val_loss: 0.6644 - val_accuracy: 0.6018\n",
            "Epoch 181/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.5751 - val_loss: 0.6643 - val_accuracy: 0.6018\n",
            "Epoch 182/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.5748 - val_loss: 0.6642 - val_accuracy: 0.6018\n",
            "Epoch 183/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6755 - accuracy: 0.5751 - val_loss: 0.6641 - val_accuracy: 0.6037\n",
            "Epoch 184/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.5754 - val_loss: 0.6640 - val_accuracy: 0.6037\n",
            "Epoch 185/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.5762 - val_loss: 0.6640 - val_accuracy: 0.6018\n",
            "Epoch 186/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.5757 - val_loss: 0.6639 - val_accuracy: 0.5982\n",
            "Epoch 187/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6752 - accuracy: 0.5759 - val_loss: 0.6638 - val_accuracy: 0.6037\n",
            "Epoch 188/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5760 - val_loss: 0.6637 - val_accuracy: 0.6018\n",
            "Epoch 189/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5765 - val_loss: 0.6636 - val_accuracy: 0.6000\n",
            "Epoch 190/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.5766 - val_loss: 0.6635 - val_accuracy: 0.6000\n",
            "Epoch 191/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5775 - val_loss: 0.6634 - val_accuracy: 0.6000\n",
            "Epoch 192/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5777 - val_loss: 0.6633 - val_accuracy: 0.6000\n",
            "Epoch 193/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5776 - val_loss: 0.6633 - val_accuracy: 0.6018\n",
            "Epoch 194/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.5778 - val_loss: 0.6632 - val_accuracy: 0.6018\n",
            "Epoch 195/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.5781 - val_loss: 0.6631 - val_accuracy: 0.5982\n",
            "Epoch 196/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6746 - accuracy: 0.5782 - val_loss: 0.6630 - val_accuracy: 0.6000\n",
            "Epoch 197/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.5784 - val_loss: 0.6629 - val_accuracy: 0.5982\n",
            "Epoch 198/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.5787 - val_loss: 0.6629 - val_accuracy: 0.6000\n",
            "Epoch 199/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6744 - accuracy: 0.5789 - val_loss: 0.6628 - val_accuracy: 0.6000\n",
            "Epoch 200/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6744 - accuracy: 0.5793 - val_loss: 0.6627 - val_accuracy: 0.6000\n",
            "Epoch 201/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6743 - accuracy: 0.5791 - val_loss: 0.6627 - val_accuracy: 0.6000\n",
            "Epoch 202/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6742 - accuracy: 0.5796 - val_loss: 0.6626 - val_accuracy: 0.6000\n",
            "Epoch 203/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6742 - accuracy: 0.5795 - val_loss: 0.6625 - val_accuracy: 0.6000\n",
            "Epoch 204/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.5793 - val_loss: 0.6624 - val_accuracy: 0.6000\n",
            "Epoch 205/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6740 - accuracy: 0.5803 - val_loss: 0.6624 - val_accuracy: 0.5982\n",
            "Epoch 206/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.5808 - val_loss: 0.6623 - val_accuracy: 0.5982\n",
            "Epoch 207/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6739 - accuracy: 0.5808 - val_loss: 0.6622 - val_accuracy: 0.5982\n",
            "Epoch 208/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5811 - val_loss: 0.6621 - val_accuracy: 0.6000\n",
            "Epoch 209/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.5811 - val_loss: 0.6620 - val_accuracy: 0.5982\n",
            "Epoch 210/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.5811 - val_loss: 0.6620 - val_accuracy: 0.5982\n",
            "Epoch 211/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6737 - accuracy: 0.5813 - val_loss: 0.6619 - val_accuracy: 0.5982\n",
            "Epoch 212/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5815 - val_loss: 0.6618 - val_accuracy: 0.5963\n",
            "Epoch 213/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5811 - val_loss: 0.6617 - val_accuracy: 0.5963\n",
            "Epoch 214/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6735 - accuracy: 0.5813 - val_loss: 0.6617 - val_accuracy: 0.5963\n",
            "Epoch 215/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6735 - accuracy: 0.5811 - val_loss: 0.6616 - val_accuracy: 0.5982\n",
            "Epoch 216/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6734 - accuracy: 0.5816 - val_loss: 0.6615 - val_accuracy: 0.5963\n",
            "Epoch 217/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6734 - accuracy: 0.5818 - val_loss: 0.6615 - val_accuracy: 0.5945\n",
            "Epoch 218/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6733 - accuracy: 0.5815 - val_loss: 0.6614 - val_accuracy: 0.5945\n",
            "Epoch 219/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6733 - accuracy: 0.5820 - val_loss: 0.6613 - val_accuracy: 0.5945\n",
            "Epoch 220/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6732 - accuracy: 0.5820 - val_loss: 0.6613 - val_accuracy: 0.5982\n",
            "Epoch 221/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6732 - accuracy: 0.5820 - val_loss: 0.6612 - val_accuracy: 0.5982\n",
            "Epoch 222/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.5816 - val_loss: 0.6612 - val_accuracy: 0.5982\n",
            "Epoch 223/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.5815 - val_loss: 0.6611 - val_accuracy: 0.5982\n",
            "Epoch 224/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.5815 - val_loss: 0.6610 - val_accuracy: 0.5982\n",
            "Epoch 225/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.5817 - val_loss: 0.6609 - val_accuracy: 0.5982\n",
            "Epoch 226/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.5816 - val_loss: 0.6609 - val_accuracy: 0.5982\n",
            "Epoch 227/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5819 - val_loss: 0.6608 - val_accuracy: 0.5982\n",
            "Epoch 228/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5823 - val_loss: 0.6608 - val_accuracy: 0.5982\n",
            "Epoch 229/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6727 - accuracy: 0.5825 - val_loss: 0.6607 - val_accuracy: 0.6000\n",
            "Epoch 230/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6727 - accuracy: 0.5826 - val_loss: 0.6607 - val_accuracy: 0.5982\n",
            "Epoch 231/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.5827 - val_loss: 0.6606 - val_accuracy: 0.6000\n",
            "Epoch 232/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.5832 - val_loss: 0.6605 - val_accuracy: 0.6018\n",
            "Epoch 233/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5831 - val_loss: 0.6605 - val_accuracy: 0.6018\n",
            "Epoch 234/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5830 - val_loss: 0.6604 - val_accuracy: 0.6037\n",
            "Epoch 235/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5836 - val_loss: 0.6604 - val_accuracy: 0.6037\n",
            "Epoch 236/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.5831 - val_loss: 0.6604 - val_accuracy: 0.6037\n",
            "Epoch 237/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.5836 - val_loss: 0.6603 - val_accuracy: 0.6037\n",
            "Epoch 238/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.5836 - val_loss: 0.6603 - val_accuracy: 0.6037\n",
            "Epoch 239/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.5828 - val_loss: 0.6602 - val_accuracy: 0.6037\n",
            "Epoch 240/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.5833 - val_loss: 0.6602 - val_accuracy: 0.6018\n",
            "Epoch 241/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.5833 - val_loss: 0.6601 - val_accuracy: 0.6018\n",
            "Epoch 242/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.5831 - val_loss: 0.6600 - val_accuracy: 0.6037\n",
            "Epoch 243/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.5840 - val_loss: 0.6600 - val_accuracy: 0.6000\n",
            "Epoch 244/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5839 - val_loss: 0.6600 - val_accuracy: 0.6000\n",
            "Epoch 245/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5836 - val_loss: 0.6599 - val_accuracy: 0.6000\n",
            "Epoch 246/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.5833 - val_loss: 0.6599 - val_accuracy: 0.6000\n",
            "Epoch 247/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.5840 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
            "Epoch 248/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.5839 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
            "Epoch 249/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6718 - accuracy: 0.5837 - val_loss: 0.6597 - val_accuracy: 0.6000\n",
            "Epoch 250/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6718 - accuracy: 0.5836 - val_loss: 0.6597 - val_accuracy: 0.6000\n",
            "Epoch 251/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6717 - accuracy: 0.5841 - val_loss: 0.6597 - val_accuracy: 0.6018\n",
            "Epoch 252/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6717 - accuracy: 0.5839 - val_loss: 0.6596 - val_accuracy: 0.5982\n",
            "Epoch 253/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.5840 - val_loss: 0.6596 - val_accuracy: 0.5963\n",
            "Epoch 254/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.5842 - val_loss: 0.6595 - val_accuracy: 0.5963\n",
            "Epoch 255/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.5838 - val_loss: 0.6595 - val_accuracy: 0.5963\n",
            "Epoch 256/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.5839 - val_loss: 0.6594 - val_accuracy: 0.5982\n",
            "Epoch 257/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6715 - accuracy: 0.5840 - val_loss: 0.6594 - val_accuracy: 0.5982\n",
            "Epoch 258/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6714 - accuracy: 0.5843 - val_loss: 0.6593 - val_accuracy: 0.5982\n",
            "Epoch 259/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6714 - accuracy: 0.5842 - val_loss: 0.6593 - val_accuracy: 0.5963\n",
            "Epoch 260/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6714 - accuracy: 0.5842 - val_loss: 0.6592 - val_accuracy: 0.5963\n",
            "Epoch 261/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.5837 - val_loss: 0.6592 - val_accuracy: 0.5963\n",
            "Epoch 262/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.5841 - val_loss: 0.6592 - val_accuracy: 0.5963\n",
            "Epoch 263/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.5840 - val_loss: 0.6592 - val_accuracy: 0.5963\n",
            "Epoch 264/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.5841 - val_loss: 0.6591 - val_accuracy: 0.5963\n",
            "Epoch 265/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.5843 - val_loss: 0.6591 - val_accuracy: 0.5963\n",
            "Epoch 266/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6711 - accuracy: 0.5847 - val_loss: 0.6590 - val_accuracy: 0.5963\n",
            "Epoch 267/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.5846 - val_loss: 0.6590 - val_accuracy: 0.5963\n",
            "Epoch 268/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.5847 - val_loss: 0.6590 - val_accuracy: 0.5963\n",
            "Epoch 269/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.5851 - val_loss: 0.6589 - val_accuracy: 0.5963\n",
            "Epoch 270/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5853 - val_loss: 0.6589 - val_accuracy: 0.5963\n",
            "Epoch 271/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5849 - val_loss: 0.6588 - val_accuracy: 0.5963\n",
            "Epoch 272/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5851 - val_loss: 0.6588 - val_accuracy: 0.5963\n",
            "Epoch 273/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5854 - val_loss: 0.6587 - val_accuracy: 0.5963\n",
            "Epoch 274/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.5853 - val_loss: 0.6587 - val_accuracy: 0.5945\n",
            "Epoch 275/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.5853 - val_loss: 0.6587 - val_accuracy: 0.5945\n",
            "Epoch 276/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.5851 - val_loss: 0.6586 - val_accuracy: 0.5945\n",
            "Epoch 277/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6707 - accuracy: 0.5855 - val_loss: 0.6586 - val_accuracy: 0.5945\n",
            "Epoch 278/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6707 - accuracy: 0.5853 - val_loss: 0.6585 - val_accuracy: 0.5945\n",
            "Epoch 279/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6707 - accuracy: 0.5853 - val_loss: 0.6585 - val_accuracy: 0.5927\n",
            "Epoch 280/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6706 - accuracy: 0.5854 - val_loss: 0.6584 - val_accuracy: 0.5927\n",
            "Epoch 281/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6706 - accuracy: 0.5850 - val_loss: 0.6584 - val_accuracy: 0.5945\n",
            "Epoch 282/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6706 - accuracy: 0.5858 - val_loss: 0.6583 - val_accuracy: 0.5945\n",
            "Epoch 283/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5849 - val_loss: 0.6583 - val_accuracy: 0.5945\n",
            "Epoch 284/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5853 - val_loss: 0.6583 - val_accuracy: 0.5927\n",
            "Epoch 285/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5855 - val_loss: 0.6583 - val_accuracy: 0.5927\n",
            "Epoch 286/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.5856 - val_loss: 0.6582 - val_accuracy: 0.5927\n",
            "Epoch 287/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6704 - accuracy: 0.5854 - val_loss: 0.6582 - val_accuracy: 0.5945\n",
            "Epoch 288/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6704 - accuracy: 0.5857 - val_loss: 0.6581 - val_accuracy: 0.5945\n",
            "Epoch 289/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6703 - accuracy: 0.5855 - val_loss: 0.6581 - val_accuracy: 0.5945\n",
            "Epoch 290/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.5855 - val_loss: 0.6581 - val_accuracy: 0.5945\n",
            "Epoch 291/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6703 - accuracy: 0.5861 - val_loss: 0.6580 - val_accuracy: 0.5945\n",
            "Epoch 292/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6702 - accuracy: 0.5860 - val_loss: 0.6580 - val_accuracy: 0.5945\n",
            "Epoch 293/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.5858 - val_loss: 0.6580 - val_accuracy: 0.5963\n",
            "Epoch 294/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.5860 - val_loss: 0.6580 - val_accuracy: 0.5963\n",
            "Epoch 295/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.5861 - val_loss: 0.6579 - val_accuracy: 0.5963\n",
            "Epoch 296/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.5863 - val_loss: 0.6579 - val_accuracy: 0.5963\n",
            "Epoch 297/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.5865 - val_loss: 0.6578 - val_accuracy: 0.5963\n",
            "Epoch 298/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6700 - accuracy: 0.5865 - val_loss: 0.6578 - val_accuracy: 0.5963\n",
            "Epoch 299/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6700 - accuracy: 0.5863 - val_loss: 0.6578 - val_accuracy: 0.5963\n",
            "Epoch 300/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6700 - accuracy: 0.5863 - val_loss: 0.6578 - val_accuracy: 0.5963\n",
            "Epoch 301/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.5863 - val_loss: 0.6577 - val_accuracy: 0.5963\n",
            "Epoch 302/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.5861 - val_loss: 0.6577 - val_accuracy: 0.5963\n",
            "Epoch 303/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.5864 - val_loss: 0.6577 - val_accuracy: 0.5963\n",
            "Epoch 304/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.5868 - val_loss: 0.6576 - val_accuracy: 0.5963\n",
            "Epoch 305/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5862 - val_loss: 0.6576 - val_accuracy: 0.5982\n",
            "Epoch 306/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5860 - val_loss: 0.6576 - val_accuracy: 0.5982\n",
            "Epoch 307/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5860 - val_loss: 0.6575 - val_accuracy: 0.5982\n",
            "Epoch 308/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6697 - accuracy: 0.5866 - val_loss: 0.6575 - val_accuracy: 0.5982\n",
            "Epoch 309/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6697 - accuracy: 0.5866 - val_loss: 0.6575 - val_accuracy: 0.6000\n",
            "Epoch 310/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5862 - val_loss: 0.6575 - val_accuracy: 0.6000\n",
            "Epoch 311/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5864 - val_loss: 0.6575 - val_accuracy: 0.6000\n",
            "Epoch 312/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.5865 - val_loss: 0.6574 - val_accuracy: 0.6000\n",
            "Epoch 313/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.5865 - val_loss: 0.6574 - val_accuracy: 0.6000\n",
            "Epoch 314/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5865 - val_loss: 0.6574 - val_accuracy: 0.6000\n",
            "Epoch 315/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.5864 - val_loss: 0.6573 - val_accuracy: 0.6000\n",
            "Epoch 316/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.5866 - val_loss: 0.6573 - val_accuracy: 0.6000\n",
            "Epoch 317/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.5867 - val_loss: 0.6573 - val_accuracy: 0.6000\n",
            "Epoch 318/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5862 - val_loss: 0.6573 - val_accuracy: 0.6000\n",
            "Epoch 319/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5864 - val_loss: 0.6572 - val_accuracy: 0.6000\n",
            "Epoch 320/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5864 - val_loss: 0.6572 - val_accuracy: 0.5982\n",
            "Epoch 321/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5864 - val_loss: 0.6572 - val_accuracy: 0.5963\n",
            "Epoch 322/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.5865 - val_loss: 0.6572 - val_accuracy: 0.5963\n",
            "Epoch 323/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5863 - val_loss: 0.6571 - val_accuracy: 0.5963\n",
            "Epoch 324/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.5867 - val_loss: 0.6571 - val_accuracy: 0.5963\n",
            "Epoch 325/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.5864 - val_loss: 0.6571 - val_accuracy: 0.5963\n",
            "Epoch 326/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6692 - accuracy: 0.5861 - val_loss: 0.6571 - val_accuracy: 0.5963\n",
            "Epoch 327/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.5865 - val_loss: 0.6570 - val_accuracy: 0.5963\n",
            "Epoch 328/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.5869 - val_loss: 0.6570 - val_accuracy: 0.5963\n",
            "Epoch 329/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.5867 - val_loss: 0.6570 - val_accuracy: 0.5963\n",
            "Epoch 330/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.5869 - val_loss: 0.6570 - val_accuracy: 0.5982\n",
            "Epoch 331/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.5871 - val_loss: 0.6569 - val_accuracy: 0.5982\n",
            "Epoch 332/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.5867 - val_loss: 0.6569 - val_accuracy: 0.6000\n",
            "Epoch 333/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.5875 - val_loss: 0.6569 - val_accuracy: 0.5982\n",
            "Epoch 334/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.5873 - val_loss: 0.6569 - val_accuracy: 0.5982\n",
            "Epoch 335/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.5870 - val_loss: 0.6568 - val_accuracy: 0.6000\n",
            "Epoch 336/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6689 - accuracy: 0.5873 - val_loss: 0.6568 - val_accuracy: 0.6000\n",
            "Epoch 337/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.5871 - val_loss: 0.6568 - val_accuracy: 0.6000\n",
            "Epoch 338/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.5873 - val_loss: 0.6568 - val_accuracy: 0.6000\n",
            "Epoch 339/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.5875 - val_loss: 0.6567 - val_accuracy: 0.5982\n",
            "Epoch 340/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.5873 - val_loss: 0.6567 - val_accuracy: 0.5982\n",
            "Epoch 341/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.5874 - val_loss: 0.6567 - val_accuracy: 0.5982\n",
            "Epoch 342/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.5871 - val_loss: 0.6567 - val_accuracy: 0.5982\n",
            "Epoch 343/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.5872 - val_loss: 0.6566 - val_accuracy: 0.5982\n",
            "Epoch 344/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.5873 - val_loss: 0.6566 - val_accuracy: 0.5982\n",
            "Epoch 345/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.5870 - val_loss: 0.6566 - val_accuracy: 0.5982\n",
            "Epoch 346/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.5880 - val_loss: 0.6566 - val_accuracy: 0.5982\n",
            "Epoch 347/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.5878 - val_loss: 0.6566 - val_accuracy: 0.5982\n",
            "Epoch 348/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.5881 - val_loss: 0.6565 - val_accuracy: 0.5982\n",
            "Epoch 349/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6686 - accuracy: 0.5884 - val_loss: 0.6565 - val_accuracy: 0.5982\n",
            "Epoch 350/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.5882 - val_loss: 0.6565 - val_accuracy: 0.6000\n",
            "Epoch 351/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.5885 - val_loss: 0.6565 - val_accuracy: 0.6000\n",
            "Epoch 352/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.5884 - val_loss: 0.6565 - val_accuracy: 0.6000\n",
            "Epoch 353/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5883 - val_loss: 0.6564 - val_accuracy: 0.6000\n",
            "Epoch 354/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5884 - val_loss: 0.6564 - val_accuracy: 0.6000\n",
            "Epoch 355/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.5887 - val_loss: 0.6563 - val_accuracy: 0.6000\n",
            "Epoch 356/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6684 - accuracy: 0.5886 - val_loss: 0.6563 - val_accuracy: 0.6000\n",
            "Epoch 357/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.5892 - val_loss: 0.6563 - val_accuracy: 0.6018\n",
            "Epoch 358/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.5893 - val_loss: 0.6563 - val_accuracy: 0.6018\n",
            "Epoch 359/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6684 - accuracy: 0.5893 - val_loss: 0.6563 - val_accuracy: 0.6018\n",
            "Epoch 360/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.5895 - val_loss: 0.6562 - val_accuracy: 0.6000\n",
            "Epoch 361/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.5896 - val_loss: 0.6562 - val_accuracy: 0.6018\n",
            "Epoch 362/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.5894 - val_loss: 0.6562 - val_accuracy: 0.6018\n",
            "Epoch 363/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6683 - accuracy: 0.5897 - val_loss: 0.6562 - val_accuracy: 0.6000\n",
            "Epoch 364/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6683 - accuracy: 0.5894 - val_loss: 0.6562 - val_accuracy: 0.6000\n",
            "Epoch 365/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.5892 - val_loss: 0.6561 - val_accuracy: 0.6000\n",
            "Epoch 366/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6682 - accuracy: 0.5895 - val_loss: 0.6561 - val_accuracy: 0.6000\n",
            "Epoch 367/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6682 - accuracy: 0.5893 - val_loss: 0.6561 - val_accuracy: 0.6000\n",
            "Epoch 368/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6682 - accuracy: 0.5896 - val_loss: 0.6561 - val_accuracy: 0.6018\n",
            "Epoch 369/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6681 - accuracy: 0.5899 - val_loss: 0.6560 - val_accuracy: 0.6018\n",
            "Epoch 370/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.5900 - val_loss: 0.6560 - val_accuracy: 0.6018\n",
            "Epoch 371/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.5898 - val_loss: 0.6560 - val_accuracy: 0.6018\n",
            "Epoch 372/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6681 - accuracy: 0.5895 - val_loss: 0.6560 - val_accuracy: 0.6018\n",
            "Epoch 373/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6681 - accuracy: 0.5894 - val_loss: 0.6559 - val_accuracy: 0.6018\n",
            "Epoch 374/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6680 - accuracy: 0.5890 - val_loss: 0.6559 - val_accuracy: 0.6018\n",
            "Epoch 375/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6680 - accuracy: 0.5895 - val_loss: 0.6559 - val_accuracy: 0.6018\n",
            "Epoch 376/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6680 - accuracy: 0.5897 - val_loss: 0.6559 - val_accuracy: 0.6018\n",
            "Epoch 377/2000\n",
            "99/99 [==============================] - 1s 7ms/step - loss: 0.6680 - accuracy: 0.5891 - val_loss: 0.6559 - val_accuracy: 0.6037\n",
            "Epoch 378/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6680 - accuracy: 0.5896 - val_loss: 0.6559 - val_accuracy: 0.6037\n",
            "Epoch 379/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5894 - val_loss: 0.6559 - val_accuracy: 0.6018\n",
            "Epoch 380/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5897 - val_loss: 0.6558 - val_accuracy: 0.6037\n",
            "Epoch 381/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.5894 - val_loss: 0.6558 - val_accuracy: 0.6055\n",
            "Epoch 382/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.5901 - val_loss: 0.6558 - val_accuracy: 0.6055\n",
            "Epoch 383/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.5901 - val_loss: 0.6558 - val_accuracy: 0.6055\n",
            "Epoch 384/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.5901 - val_loss: 0.6558 - val_accuracy: 0.6055\n",
            "Epoch 385/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.5900 - val_loss: 0.6558 - val_accuracy: 0.6055\n",
            "Epoch 386/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.5898 - val_loss: 0.6557 - val_accuracy: 0.6055\n",
            "Epoch 387/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.5906 - val_loss: 0.6557 - val_accuracy: 0.6055\n",
            "Epoch 388/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5906 - val_loss: 0.6557 - val_accuracy: 0.6055\n",
            "Epoch 389/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5908 - val_loss: 0.6557 - val_accuracy: 0.6055\n",
            "Epoch 390/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5905 - val_loss: 0.6557 - val_accuracy: 0.6018\n",
            "Epoch 391/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5907 - val_loss: 0.6557 - val_accuracy: 0.6018\n",
            "Epoch 392/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5906 - val_loss: 0.6557 - val_accuracy: 0.6055\n",
            "Epoch 393/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5917 - val_loss: 0.6557 - val_accuracy: 0.6018\n",
            "Epoch 394/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5914 - val_loss: 0.6556 - val_accuracy: 0.6037\n",
            "Epoch 395/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5919 - val_loss: 0.6556 - val_accuracy: 0.6037\n",
            "Epoch 396/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5918 - val_loss: 0.6556 - val_accuracy: 0.6073\n",
            "Epoch 397/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5919 - val_loss: 0.6556 - val_accuracy: 0.6055\n",
            "Epoch 398/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5922 - val_loss: 0.6556 - val_accuracy: 0.6037\n",
            "Epoch 399/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5919 - val_loss: 0.6556 - val_accuracy: 0.6055\n",
            "Epoch 400/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5922 - val_loss: 0.6556 - val_accuracy: 0.6092\n",
            "Epoch 401/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5916 - val_loss: 0.6555 - val_accuracy: 0.6092\n",
            "Epoch 402/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5913 - val_loss: 0.6555 - val_accuracy: 0.6092\n",
            "Epoch 403/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6674 - accuracy: 0.5909 - val_loss: 0.6555 - val_accuracy: 0.6092\n",
            "Epoch 404/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.5912 - val_loss: 0.6555 - val_accuracy: 0.6073\n",
            "Epoch 405/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6674 - accuracy: 0.5910 - val_loss: 0.6555 - val_accuracy: 0.6092\n",
            "Epoch 406/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.5913 - val_loss: 0.6555 - val_accuracy: 0.6092\n",
            "Epoch 407/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.5914 - val_loss: 0.6554 - val_accuracy: 0.6092\n",
            "Epoch 408/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.5914 - val_loss: 0.6554 - val_accuracy: 0.6092\n",
            "Epoch 409/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.5915 - val_loss: 0.6554 - val_accuracy: 0.6037\n",
            "Epoch 410/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.5916 - val_loss: 0.6554 - val_accuracy: 0.6037\n",
            "Epoch 411/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.5916 - val_loss: 0.6554 - val_accuracy: 0.6018\n",
            "Epoch 412/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.5915 - val_loss: 0.6554 - val_accuracy: 0.6018\n",
            "Epoch 413/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5914 - val_loss: 0.6554 - val_accuracy: 0.6018\n",
            "Epoch 414/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5918 - val_loss: 0.6554 - val_accuracy: 0.6018\n",
            "Epoch 415/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5915 - val_loss: 0.6554 - val_accuracy: 0.6018\n",
            "Epoch 416/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.5920 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 417/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5927 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 418/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5922 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 419/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5926 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 420/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.5924 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 421/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5924 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 422/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5927 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 423/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5930 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 424/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5931 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 425/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6670 - accuracy: 0.5930 - val_loss: 0.6553 - val_accuracy: 0.6018\n",
            "Epoch 426/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5935 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 427/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5934 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 428/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6670 - accuracy: 0.5933 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 429/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5936 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 430/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.5936 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 431/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.5935 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 432/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.5937 - val_loss: 0.6552 - val_accuracy: 0.6018\n",
            "Epoch 433/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.5939 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 434/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.5936 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 435/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.5939 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 436/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.5938 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 437/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.5938 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 438/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.5932 - val_loss: 0.6551 - val_accuracy: 0.6018\n",
            "Epoch 439/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.5938 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 440/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.5940 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 441/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.5938 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 442/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6667 - accuracy: 0.5939 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 443/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6667 - accuracy: 0.5940 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 444/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6667 - accuracy: 0.5940 - val_loss: 0.6550 - val_accuracy: 0.6018\n",
            "Epoch 445/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.5938 - val_loss: 0.6549 - val_accuracy: 0.6018\n",
            "Epoch 446/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6667 - accuracy: 0.5929 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 447/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.5927 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 448/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.5931 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 449/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.5931 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 450/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.5929 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 451/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.5929 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 452/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.5930 - val_loss: 0.6548 - val_accuracy: 0.6000\n",
            "Epoch 453/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.5931 - val_loss: 0.6548 - val_accuracy: 0.6000\n",
            "Epoch 454/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5926 - val_loss: 0.6548 - val_accuracy: 0.6000\n",
            "Epoch 455/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.5934 - val_loss: 0.6548 - val_accuracy: 0.5982\n",
            "Epoch 456/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5933 - val_loss: 0.6548 - val_accuracy: 0.5982\n",
            "Epoch 457/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5938 - val_loss: 0.6548 - val_accuracy: 0.5982\n",
            "Epoch 458/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5941 - val_loss: 0.6548 - val_accuracy: 0.5963\n",
            "Epoch 459/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.5939 - val_loss: 0.6548 - val_accuracy: 0.5963\n",
            "Epoch 460/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.5944 - val_loss: 0.6548 - val_accuracy: 0.5963\n",
            "Epoch 461/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5936 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 462/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5941 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 463/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5940 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 464/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5938 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 465/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5939 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 466/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5941 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 467/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5941 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 468/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.5939 - val_loss: 0.6547 - val_accuracy: 0.5982\n",
            "Epoch 469/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5942 - val_loss: 0.6546 - val_accuracy: 0.5982\n",
            "Epoch 470/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5941 - val_loss: 0.6546 - val_accuracy: 0.5982\n",
            "Epoch 471/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5940 - val_loss: 0.6546 - val_accuracy: 0.5982\n",
            "Epoch 472/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5941 - val_loss: 0.6546 - val_accuracy: 0.5982\n",
            "Epoch 473/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5941 - val_loss: 0.6546 - val_accuracy: 0.6000\n",
            "Epoch 474/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5942 - val_loss: 0.6546 - val_accuracy: 0.6000\n",
            "Epoch 475/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6662 - accuracy: 0.5944 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
            "Epoch 476/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5941 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
            "Epoch 477/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5943 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
            "Epoch 478/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5944 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
            "Epoch 479/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.5943 - val_loss: 0.6545 - val_accuracy: 0.6018\n",
            "Epoch 480/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.5943 - val_loss: 0.6545 - val_accuracy: 0.6018\n",
            "Epoch 481/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.5942 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
            "Epoch 482/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.5943 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 483/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6661 - accuracy: 0.5944 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 484/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6661 - accuracy: 0.5941 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 485/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6661 - accuracy: 0.5944 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 486/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.5943 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 487/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6660 - accuracy: 0.5944 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 488/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6660 - accuracy: 0.5944 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 489/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.5942 - val_loss: 0.6544 - val_accuracy: 0.6018\n",
            "Epoch 490/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6660 - accuracy: 0.5945 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 491/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.5939 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 492/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5941 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 493/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5939 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 494/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5940 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 495/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5943 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 496/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5937 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 497/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6659 - accuracy: 0.5938 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 498/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.5940 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 499/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 500/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.5939 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 501/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5940 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 502/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 503/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 504/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5939 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 505/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5942 - val_loss: 0.6543 - val_accuracy: 0.6018\n",
            "Epoch 506/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5943 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 507/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5942 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 508/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5944 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 509/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5945 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 510/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5943 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 511/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5945 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 512/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5948 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 513/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5948 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 514/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5950 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 515/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5950 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 516/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.5950 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 517/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5950 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 518/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5949 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 519/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5951 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 520/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5948 - val_loss: 0.6542 - val_accuracy: 0.6018\n",
            "Epoch 521/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5947 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 522/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5947 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 523/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5949 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 524/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6655 - accuracy: 0.5949 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 525/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5948 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 526/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.5944 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 527/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.5948 - val_loss: 0.6541 - val_accuracy: 0.6037\n",
            "Epoch 528/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5948 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 529/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5948 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 530/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5947 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 531/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5949 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 532/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5948 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 533/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5946 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 534/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5949 - val_loss: 0.6541 - val_accuracy: 0.6018\n",
            "Epoch 535/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5949 - val_loss: 0.6540 - val_accuracy: 0.6018\n",
            "Epoch 536/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5952 - val_loss: 0.6540 - val_accuracy: 0.6037\n",
            "Epoch 537/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5955 - val_loss: 0.6540 - val_accuracy: 0.6055\n",
            "Epoch 538/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5953 - val_loss: 0.6540 - val_accuracy: 0.6055\n",
            "Epoch 539/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5957 - val_loss: 0.6540 - val_accuracy: 0.6055\n",
            "Epoch 540/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5952 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 541/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5958 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 542/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5956 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 543/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 544/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5960 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 545/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5956 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 546/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 547/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5961 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 548/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5960 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 549/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 550/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5959 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 551/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 552/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 553/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5961 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 554/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5958 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 555/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 556/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5963 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 557/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 558/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5959 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 559/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5960 - val_loss: 0.6540 - val_accuracy: 0.6073\n",
            "Epoch 560/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5962 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 561/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5960 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 562/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5960 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 563/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5963 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 564/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5965 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 565/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5967 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 566/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5967 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 567/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 568/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5969 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 569/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 570/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 571/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5969 - val_loss: 0.6539 - val_accuracy: 0.6092\n",
            "Epoch 572/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5972 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 573/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 574/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5971 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 575/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5968 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 576/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 577/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5971 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 578/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.5970 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 579/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5972 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 580/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5973 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 581/2000\n",
            "99/99 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.5972 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 582/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5971 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 583/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5975 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 584/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5971 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 585/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5971 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 586/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6647 - accuracy: 0.5971 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 587/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5973 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 588/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5973 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 589/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5975 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 590/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5976 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 591/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5978 - val_loss: 0.6539 - val_accuracy: 0.6073\n",
            "Epoch 592/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5976 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 593/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5978 - val_loss: 0.6538 - val_accuracy: 0.6073\n",
            "Epoch 594/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 595/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.5981 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 596/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 597/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5980 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 598/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 599/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5980 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 600/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5978 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 601/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 602/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 603/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.5978 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 604/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5975 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 605/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5976 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 606/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5977 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 607/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5979 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 608/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5974 - val_loss: 0.6538 - val_accuracy: 0.6092\n",
            "Epoch 609/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5972 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 610/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5976 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 611/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5981 - val_loss: 0.6538 - val_accuracy: 0.6110\n",
            "Epoch 612/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 613/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 614/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 615/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 616/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5983 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 617/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 618/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5983 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 619/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 620/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 621/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 622/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5980 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 623/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 624/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5980 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 625/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 626/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 627/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5987 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 628/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5985 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 629/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5983 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 630/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 631/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 632/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 633/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5982 - val_loss: 0.6537 - val_accuracy: 0.6110\n",
            "Epoch 634/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5985 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 635/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 636/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5981 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 637/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 638/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.5985 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 639/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5984 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 640/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5983 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 641/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5985 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 642/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5986 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 643/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5985 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 644/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5987 - val_loss: 0.6536 - val_accuracy: 0.6092\n",
            "Epoch 645/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5989 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 646/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5988 - val_loss: 0.6536 - val_accuracy: 0.6092\n",
            "Epoch 647/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.5986 - val_loss: 0.6536 - val_accuracy: 0.6092\n",
            "Epoch 648/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6639 - accuracy: 0.5987 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 649/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6639 - accuracy: 0.5988 - val_loss: 0.6536 - val_accuracy: 0.6092\n",
            "Epoch 650/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5991 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 651/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5990 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 652/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5989 - val_loss: 0.6536 - val_accuracy: 0.6110\n",
            "Epoch 653/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6110\n",
            "Epoch 654/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6110\n",
            "Epoch 655/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6110\n",
            "Epoch 656/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5989 - val_loss: 0.6536 - val_accuracy: 0.6092\n",
            "Epoch 657/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6092\n",
            "Epoch 658/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5991 - val_loss: 0.6535 - val_accuracy: 0.6110\n",
            "Epoch 659/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6092\n",
            "Epoch 660/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5987 - val_loss: 0.6535 - val_accuracy: 0.6092\n",
            "Epoch 661/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 662/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 663/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5985 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 664/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5988 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 665/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5989 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 666/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5985 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 667/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5985 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 668/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5984 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 669/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5987 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 670/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 671/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 672/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5987 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 673/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 674/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5986 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 675/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5988 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 676/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5987 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 677/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5990 - val_loss: 0.6535 - val_accuracy: 0.6073\n",
            "Epoch 678/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 679/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 680/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 681/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5992 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 682/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5985 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 683/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5992 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 684/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.5988 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 685/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5986 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 686/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 687/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5988 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 688/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5988 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 689/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5986 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 690/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 691/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5986 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 692/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 693/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5986 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 694/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 695/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6110\n",
            "Epoch 696/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5987 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 697/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5992 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 698/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 699/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 700/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5989 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 701/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5992 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 702/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5991 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 703/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5990 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 704/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6634 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6110\n",
            "Epoch 705/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6110\n",
            "Epoch 706/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 707/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 708/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 709/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 710/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 711/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 712/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 713/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5991 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 714/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.5988 - val_loss: 0.6534 - val_accuracy: 0.6092\n",
            "Epoch 715/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 716/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 717/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5991 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 718/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5992 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 719/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 720/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5990 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 721/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 722/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5990 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 723/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 724/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5989 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 725/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5988 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 726/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5985 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 727/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 728/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5987 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 729/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5985 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 730/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5984 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 731/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5984 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 732/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5983 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 733/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6631 - accuracy: 0.5987 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 734/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6630 - accuracy: 0.5985 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 735/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5982 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 736/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5984 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 737/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5983 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 738/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5982 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 739/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5985 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 740/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5984 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 741/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5980 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 742/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5980 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 743/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5983 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 744/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.5981 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 745/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5984 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 746/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5985 - val_loss: 0.6533 - val_accuracy: 0.6092\n",
            "Epoch 747/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5986 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 748/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5985 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 749/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5982 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 750/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5987 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 751/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5985 - val_loss: 0.6532 - val_accuracy: 0.6092\n",
            "Epoch 752/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5980 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 753/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5983 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 754/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.5986 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 755/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.5982 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 756/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.5984 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 757/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.5982 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 758/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.5985 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 759/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.5985 - val_loss: 0.6532 - val_accuracy: 0.6055\n",
            "Epoch 760/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.5989 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 761/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6628 - accuracy: 0.5988 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 762/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.5990 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 763/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6628 - accuracy: 0.5983 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 764/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6628 - accuracy: 0.5986 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 765/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6628 - accuracy: 0.5988 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 766/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 767/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6627 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 768/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6627 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 769/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 770/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5989 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 771/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 772/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 773/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 774/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5988 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 775/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5989 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 776/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 777/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5990 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 778/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5993 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 779/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 780/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 781/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 782/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 783/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5993 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 784/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5993 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 785/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5987 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 786/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 787/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 788/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5990 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 789/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 790/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6037\n",
            "Epoch 791/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5989 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 792/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5988 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 793/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5986 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 794/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5990 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 795/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 796/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 797/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5994 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 798/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 799/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 800/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5991 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 801/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 802/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5992 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 803/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 804/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 805/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5994 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 806/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 807/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5994 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 808/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5994 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 809/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5993 - val_loss: 0.6531 - val_accuracy: 0.6055\n",
            "Epoch 810/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5988 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 811/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5997 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 812/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5992 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 813/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5994 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 814/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5996 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 815/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5995 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 816/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5995 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 817/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5996 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 818/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5997 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 819/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5996 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 820/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5995 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 821/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5991 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 822/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5996 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 823/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5997 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 824/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5995 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 825/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5995 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 826/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5997 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 827/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6000 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 828/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6002 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 829/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6004 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 830/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5998 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 831/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6002 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 832/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6002 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 833/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6000 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 834/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.5999 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 835/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.5999 - val_loss: 0.6530 - val_accuracy: 0.6055\n",
            "Epoch 836/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6003 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
            "Epoch 837/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6002 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
            "Epoch 838/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6003 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
            "Epoch 839/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6002 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
            "Epoch 840/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6004 - val_loss: 0.6530 - val_accuracy: 0.6037\n",
            "Epoch 841/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6001 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 842/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.5998 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 843/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 844/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6001 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 845/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 846/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6001 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 847/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 848/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 849/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6001 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 850/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 851/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 852/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6006 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 853/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6002 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 854/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6037\n",
            "Epoch 855/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 856/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6002 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 857/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 858/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6006 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 859/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6009 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 860/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6007 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 861/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6008 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 862/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6006 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 863/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6007 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 864/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 865/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6003 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 866/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6005 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 867/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6006 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 868/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6004 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 869/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6006 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 870/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6003 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 871/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6007 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 872/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6004 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 873/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6004 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 874/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6004 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 875/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6007 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 876/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6004 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 877/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6005 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 878/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6007 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 879/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6005 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 880/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6006 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 881/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6007 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 882/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6004 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 883/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6007 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 884/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6008 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 885/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6007 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 886/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6011 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 887/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6011 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 888/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6011 - val_loss: 0.6529 - val_accuracy: 0.6018\n",
            "Epoch 889/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6009 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 890/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6010 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 891/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6009 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 892/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6009 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 893/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6013 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 894/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6010 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 895/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6013 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 896/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6011 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 897/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6017 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 898/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6019 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 899/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6013 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 900/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 901/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 902/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6012 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 903/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6020 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 904/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6015 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 905/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6017 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 906/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.6014 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 907/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6015 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 908/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 909/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 910/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6021 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 911/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6019 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 912/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 913/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6020 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 914/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6018 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 915/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6017 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 916/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6023 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 917/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6023 - val_loss: 0.6528 - val_accuracy: 0.6018\n",
            "Epoch 918/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6028 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 919/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6023 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 920/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6021 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 921/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6021 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 922/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6022 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 923/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6019 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 924/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6025 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 925/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6021 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
            "Epoch 926/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6024 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 927/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6024 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 928/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6022 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 929/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6024 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 930/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6024 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 931/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6022 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 932/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6023 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 933/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6020 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 934/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6030 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 935/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6023 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 936/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6027 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 937/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6021 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 938/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6020 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 939/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6026 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 940/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6025 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 941/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6024 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 942/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6023 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 943/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6019 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 944/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6015 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 945/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6019 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 946/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6019 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 947/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6022 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 948/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6027 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 949/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6028 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 950/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6029 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 951/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6030 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 952/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6025 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 953/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6026 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 954/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6027 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 955/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6027 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 956/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6022 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
            "Epoch 957/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.6022 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 958/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 959/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 960/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 961/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6023 - val_loss: 0.6527 - val_accuracy: 0.5982\n",
            "Epoch 962/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6023 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 963/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 964/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6028 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 965/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 966/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 967/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6023 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 968/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6022 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 969/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6025 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 970/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 971/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 972/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6025 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 973/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 974/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 975/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6028 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 976/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6025 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 977/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 978/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 979/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6022 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 980/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 981/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 982/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6025 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 983/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6028 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 984/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6029 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 985/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 986/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 987/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6023 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 988/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 989/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 990/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6024 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 991/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6024 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 992/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6024 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 993/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 994/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 995/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6025 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 996/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 997/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6029 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 998/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6031 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 999/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6031 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1000/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6027 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1001/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6028 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1002/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1003/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6026 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1004/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6032 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1005/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6028 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1006/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6028 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1007/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1008/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6029 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 1009/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6030 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1010/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6028 - val_loss: 0.6526 - val_accuracy: 0.5982\n",
            "Epoch 1011/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6026 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
            "Epoch 1012/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6024 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1013/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6026 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1014/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6027 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1015/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1016/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1017/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1018/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6026 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1019/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1020/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1021/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6027 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1022/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6028 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1023/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1024/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6025 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1025/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6027 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1026/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6606 - accuracy: 0.6024 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1027/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6026 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1028/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6027 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1029/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1030/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1031/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1032/2000\n",
            "99/99 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1033/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6606 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1034/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1035/2000\n",
            "99/99 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.6027 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1036/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6028 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1037/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6028 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1038/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1039/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1040/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1041/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1042/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1043/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1044/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1045/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1046/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1047/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1048/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1049/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1050/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1051/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1052/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1053/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.6000\n",
            "Epoch 1054/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1055/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1056/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1057/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1058/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1059/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1060/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1061/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1062/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6036 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1063/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1064/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1065/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1066/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1067/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1068/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1069/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1070/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1071/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1072/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1073/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1074/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1075/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1076/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6036 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1077/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1078/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6034 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1079/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6033 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1080/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5982\n",
            "Epoch 1081/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5982\n",
            "Epoch 1082/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6030 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1083/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6030 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1084/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6030 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1085/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1086/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6032 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1087/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6029 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1088/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6031 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1089/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6029 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1090/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6029 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1091/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1092/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6036 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1093/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6035 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1094/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6036 - val_loss: 0.6525 - val_accuracy: 0.5963\n",
            "Epoch 1095/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6030 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1096/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6033 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1097/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6032 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1098/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1099/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6032 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1100/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6033 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1101/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6030 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1102/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1103/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1104/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1105/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6035 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1106/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6033 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1107/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6030 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1108/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1109/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6034 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1110/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6035 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1111/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6034 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1112/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6033 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1113/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6036 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1114/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6038 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1115/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6035 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1116/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6034 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1117/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6038 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1118/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6037 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1119/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6037 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1120/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6031 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1121/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6035 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1122/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6037 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1123/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6034 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1124/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6036 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1125/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6036 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1126/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6037 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1127/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6034 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1128/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6038 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1129/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6036 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1130/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6036 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1131/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6041 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1132/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6041 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1133/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6039 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1134/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6038 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1135/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6040 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1136/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6041 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1137/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6040 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1138/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1139/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1140/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1141/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6043 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1142/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1143/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6041 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1144/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1145/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1146/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6039 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1147/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1148/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6041 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1149/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6040 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1150/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6043 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1151/2000\n",
            "99/99 [==============================] - 0s 5ms/step - loss: 0.6597 - accuracy: 0.6043 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1152/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6044 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1153/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6047 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1154/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6043 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1155/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6043 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1156/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6045 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1157/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6045 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1158/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6047 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1159/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6043 - val_loss: 0.6524 - val_accuracy: 0.5963\n",
            "Epoch 1160/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1161/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6047 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1162/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1163/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5963\n",
            "Epoch 1164/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1165/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1166/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6040 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1167/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1168/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6041 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1169/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6596 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1170/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6046 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1171/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6047 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1172/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1173/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6047 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1174/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6043 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1175/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1176/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1177/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1178/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6046 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1179/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1180/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1181/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1182/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1183/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1184/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.5982\n",
            "Epoch 1185/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1186/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6042 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1187/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1188/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1189/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1190/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1191/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1192/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6043 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1193/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6046 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1194/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1195/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1196/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6043 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1197/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6593 - accuracy: 0.6045 - val_loss: 0.6523 - val_accuracy: 0.6000\n",
            "Epoch 1198/2000\n",
            "99/99 [==============================] - 0s 4ms/step - loss: 0.6593 - accuracy: 0.6044 - val_loss: 0.6523 - val_accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train loss / valid loss / train accuracy / valid accuracy 확인하기\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist.history['loss'], 'y', label = 'train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label = 'val loss')\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label = 'train accuracy')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label = 'val accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "loss_ax.legend(loc = 'upper left')\n",
        "acc_ax.legend(loc = 'lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ppuuJ8sqMGGn",
        "outputId": "4439ea4a-005e-45ac-e521-379305fc8af1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN model accuracy & loss 확인하기\n",
        "performance = model.evaluate(x_test, y_test, batch_size=1, verbose=1)\n",
        "print('Accuracy :', performance[1], '\\nLoss :', performance[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJvrSNofuM4q",
        "outputId": "a793e794-5c08-4939-8f7a-63ebbedac1c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "545/545 [==============================] - 2s 2ms/step - loss: 0.6441 - accuracy: 0.6220\n",
            "Accuracy : 0.6220183372497559 \n",
            "Loss : 0.6440966129302979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN model 저장하기\n",
        "model.save('./DNN_winlose_model.h5')"
      ],
      "metadata": {
        "id": "QJEPIA7V0tPr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflowjs==3.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_tltj1Y_FE6",
        "outputId": "bc23be75-7bd2-43c5-a3e0-11b4132d25cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflowjs==3.9.0\n",
            "  Downloading tensorflowjs-3.9.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs==3.9.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs==3.9.0) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs==3.9.0) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (14.0.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.26.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs==3.9.0) (3.2.0)\n",
            "Installing collected packages: tensorflowjs\n",
            "Successfully installed tensorflowjs-3.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip install tensorflowjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duK-gD_-CRcH",
        "outputId": "3841a925-d708-4798-e5c5-2f88f7dadf20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.7/dist-packages (3.9.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.47.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (14.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs"
      ],
      "metadata": {
        "id": "Ey0_jI20ADF-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfjs.converters.save_keras_model(model, 'models')"
      ],
      "metadata": {
        "id": "elpBHSOF-YB6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qaKwcR52BmLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}